{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 3: Spark Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credits and Referecens:\n",
    "\n",
    "- Learning Spark by Holden Karau, Andy Konwinski, Patrick Wendell, and Matei Zaharia (O’Reilly). Copyright 2015 Databricks, 978-1-449-35862-4.\n",
    "- Spark: The Definitive Guide by Bill Chambers and Matei Zaharia (O’Reilly). Copyright 2018 Databricks, Inc., 978-1-491-91221-8.”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.- Getting started: The SparkSession and the Spark UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "# Change the path to the Spark folder accordingly\n",
    "findspark.init(spark_home=\"/home/ubuntu/software/spark-2.2.1-bin-hadoop2.7/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"/home/ubuntu/movielens_v2/movielens/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we can import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import numpy as np # We'll be using numpy for some numeric operations\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark provides a SparkContext\n",
    "sc = pyspark.SparkContext(master=\"local[*]\", appName=\"tour\")\n",
    "# Now you can go to http://localhost:4040/ and see the Spark UI!\n",
    "# Try re-running this line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To try the SparkContext with other masters first stop the one that is already running\n",
    "# sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **local**: Run Spark locally with one worker thread (i.e. no parallelism at all).\n",
    "- **local[K]**: Run Spark locally with K worker threads (ideally, set this to the number of cores on your machine).\n",
    "- **local[*]**: Run Spark locally with as many worker threads as logical cores on your machine.\n",
    "- **spark://HOST:PORT**: Connect to the given Spark standalone cluster master. The port must be whichever one your master is configured to use, which is 7077 by default.\n",
    "- **mesos://HOST:PORT**: Connect to the given Mesos cluster. The port must be whichever one your is configured to use, which is 5050 by default. Or, for a Mesos cluster using ZooKeeper, use mesos://zk://.... To submit with --deploy-mode cluster, the HOST:PORT should be configured to connect to the MesosClusterDispatcher.\n",
    "- **yarn**: Connect to a YARN cluster in client or cluster mode depending on the value of --deploy-mode. The cluster location will be found based on the HADOOP_CONF_DIR or YARN_CONF_DIR variable.\n",
    "- **yarn-client**: Equivalent to yarn with --deploy-mode client, which is preferred to `yarn-client`\n",
    "- **yarn-cluster**: Equivalent to yarn with --deploy-mode cluster, which is preferred to `yarn-cluster`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating RDDS\n",
    "\n",
    "We saw that we can create RDDs by loading files from disk. We can also create RDDs from Python collections or transforming other RDDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method parallelize in module pyspark.context:\n",
      "\n",
      "parallelize(c, numSlices=None) method of pyspark.context.SparkContext instance\n",
      "    Distribute a local Python collection to form an RDD. Using xrange\n",
      "    is recommended if the input represents a range for performance.\n",
      "    \n",
      "    >>> sc.parallelize([0, 2, 3, 4, 6], 5).glom().collect()\n",
      "    [[0], [2], [3], [4], [6]]\n",
      "    >>> sc.parallelize(xrange(0, 6, 2), 5).glom().collect()\n",
      "    [[], [0], [], [2], [4]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sc.parallelize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an RDD from in-memory objects:\n",
    "l_numbers = np.arange(0,100000)\n",
    "numbers = sc.parallelize(l_numbers) # creation of RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = sc.textFile(os.path.join(data_folder, \"ratings.csv\")).filter(lambda x: \"movie_id\" not in x) # load data from a file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD Transformations and Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two types of RDD operations in Spark: **transformations** and **actions**.\n",
    "- Transfromations: Create new RDDs from other RDDs. \n",
    "- Actions: Extract information from RDDs and return it to the driver program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method map in module pyspark.rdd:\n",
      "\n",
      "map(f, preservesPartitioning=False) method of pyspark.rdd.PipelinedRDD instance\n",
      "    Return a new RDD by applying a function to each element of this RDD.\n",
      "    \n",
      "    >>> rdd = sc.parallelize([\"b\", \"a\", \"c\"])\n",
      "    >>> sorted(rdd.map(lambda x: (x, 1)).collect())\n",
      "    [('a', 1), ('b', 1), ('c', 1)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(ratings.map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_splitted = ratings.map(lambda x: x.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[3] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_splitted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method take in module pyspark.rdd:\n",
      "\n",
      "take(num) method of pyspark.rdd.PipelinedRDD instance\n",
      "    Take the first num elements of the RDD.\n",
      "    \n",
      "    It works by first scanning one partition, and use the results from\n",
      "    that partition to estimate the number of additional partitions needed\n",
      "    to satisfy the limit.\n",
      "    \n",
      "    Translated from the Scala implementation in RDD#take().\n",
      "    \n",
      "    .. note:: this method should only be used if the resulting array is expected\n",
      "        to be small, as all the data is loaded into the driver's memory.\n",
      "    \n",
      "    >>> sc.parallelize([2, 3, 4, 5, 6]).cache().take(2)\n",
      "    [2, 3]\n",
      "    >>> sc.parallelize([2, 3, 4, 5, 6]).take(10)\n",
      "    [2, 3, 4, 5, 6]\n",
      "    >>> sc.parallelize(range(100), 100).filter(lambda x: x > 90).take(3)\n",
      "    [91, 92, 93]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(ratings.take)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method collect in module pyspark.rdd:\n",
      "\n",
      "collect() method of pyspark.rdd.PipelinedRDD instance\n",
      "    Return a list that contains all of the elements in this RDD.\n",
      "    \n",
      "    .. note:: This method should only be used if the resulting array is expected\n",
      "        to be small, as all the data is loaded into the driver's memory.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(ratings.collect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method count in module pyspark.rdd:\n",
      "\n",
      "count() method of pyspark.rdd.PipelinedRDD instance\n",
      "    Return the number of elements in this RDD.\n",
      "    \n",
      "    >>> sc.parallelize([2, 3, 4]).count()\n",
      "    3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(ratings.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_splitted_top = ratings_splitted.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_splitted_res = ratings_splitted.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_count = ratings_splitted.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000209"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Lambda expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Lambda expressions](https://docs.python.org/3.5/howto/functional.html#small-functions-and-the-lambda-expression) are an easy way to write short functions in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda line: 'Spark' in line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(\"we are learning park\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(line):\n",
    "    return 'Spark' in line\n",
    "f(\"we are learning Spark\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's try to get the zombie movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbpedia_movies = sc.textFile(os.path.join(data_folder, \"dbpedia.csv\")) # load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count only lines that mention \"Spark\"\n",
    "zombie_movies= dbpedia_movies.filter(lambda line: 'zombie' in line).map(lambda x: x.split(\",\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[8] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zombie_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Last Dance (1996)',\n",
       " 'Braindead (1992)',\n",
       " 'House (1986)',\n",
       " 'Night of the Comet (1984)',\n",
       " 'Phantasm (1979)',\n",
       " 'Night of the Creeps (1986)']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zombie_movies.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lazy evaluation\n",
    "\n",
    "RDDs are **lazy**. This means that Spark will not materialize an RDD until it has to perform an action. In the example below, `primesRDD` is not evaluated until action `collect()` is performed on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_prime(num):\n",
    "    \"\"\" return True if num is prime, False otherwise\"\"\"\n",
    "    if num < 1 or num % 1 != 0:\n",
    "        raise Exception(\"invalid argument\")\n",
    "    for d in range(2, int(np.sqrt(num) + 1)):\n",
    "        if num % d == 0:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbersRDD = sc.parallelize(range(1, 1000000)) # creation of RDD\n",
    "primesRDD = numbersRDD.filter(is_prime) # transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# primesRDD has not been materialized until this point\n",
    "primes = primesRDD.collect() # action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43]\n",
      "[1, 2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43]\n"
     ]
    }
   ],
   "source": [
    "print(primes[0:15])\n",
    "print(primesRDD.take(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persistence\n",
    "\n",
    "RDDs are **ephemeral** by default, i.e. there is no guarantee they will remain in memory after they are materialized. If we want them to `persist` in memory, possibly to query them repeatedly or use them in multiple operations, we can ask Spark to do this by calling `persist()` on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "primesRDD_persisted = numbersRDD.filter(is_prime).persist() # transformation # we're asking Spark to keep this RDD in memory. Note that cache is the same but as using persist for memory. However, persist allows you to define other types of storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 78499 prime numbers\n",
      "Here are some of them:\n"
     ]
    }
   ],
   "source": [
    "print(\"Found\", primesRDD_persisted.count(), \"prime numbers\") # first action -- causes primesRDD_persisted to be materialized\n",
    "print(\"Here are some of them:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67]\n"
     ]
    }
   ],
   "source": [
    "print(primesRDD_persisted.collect()[0:20]) # second action - RDD is already in memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How long does it take to collect `primesRDD`? Let's time the operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.98 s ± 247 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "primes = primesRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It took about 1.8s. That's because Spark had to evaluate `primesRDD` before performing `collect` on it.\n",
    "\n",
    "How long would it take if `primesRDD_persisted` was already in memory?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.3 ms ± 6.34 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "primes = primesRDD_persisted.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It took about 20ms to collect `primesRDD_persisted`!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## map and flatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = sc.textFile(os.path.join(data_folder, \"dbpedia.csv\")).filter(lambda x: \"movie_id\" not in x)\n",
    "\n",
    "words_map = words.map(lambda phrase: phrase.split(\" \"))\n",
    "\n",
    "l_words = words_map.collect() # This returns a list of lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3,Grumpier',\n",
       " 'Old',\n",
       " 'Men',\n",
       " '(1995),http://dbpedia.org/resource/Grumpier_Old_Men,http://dbpedia.org/data/Grumpier_Old_Men.json,\"{\"\"abstract\"\":',\n",
       " '\"\"Grumpier',\n",
       " 'Old',\n",
       " 'Men',\n",
       " 'is',\n",
       " 'a',\n",
       " '1995']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_words[1][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2,Jumanji',\n",
       " '(1995),http://dbpedia.org/resource/Jumanji,http://dbpedia.org/data/Jumanji.json,\"{\"\"abstract\"\":',\n",
       " '\"\"Jumanji',\n",
       " 'is',\n",
       " 'a',\n",
       " '1995',\n",
       " 'American',\n",
       " 'family',\n",
       " 'adventure',\n",
       " 'film']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_flatmap = words.flatMap(lambda phrase: phrase.split(\" \"))\n",
    "words_flatmap.collect()[0:10] # This returns a list withe the combined elements of the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('(1995),http://dbpedia.org/resource/Jumanji,http://dbpedia.org/data/Jumanji.json,\"{\"\"abstract\"\":',\n",
       "  1),\n",
       " ('is', 6302),\n",
       " ('1995', 316),\n",
       " ('American', 2176),\n",
       " ('family', 156),\n",
       " ('It', 2501),\n",
       " ('an', 1377),\n",
       " ('adaptation', 190),\n",
       " ('of', 9920),\n",
       " ('1981', 64)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can use the flatmap to make a word count\n",
    "words_flatmap.map(\n",
    "    lambda x: (x,1)\n",
    ").reduceByKey(\n",
    "    lambda x,y: x+y\n",
    ").collect()[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Set operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[23] at parallelize at PythonRDD.scala:489"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oneRDD = sc.parallelize([1, 1, 1, 2, 3, 3, 4, 4])\n",
    "oneRDD.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[24] at parallelize at PythonRDD.scala:489"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "otherRDD = sc.parallelize([1, 4, 4, 7])\n",
    "otherRDD.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UnionRDD[25] at union at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unionRDD = oneRDD.union(otherRDD)\n",
    "unionRDD.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 3]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oneRDD.subtract(otherRDD).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oneRDD.distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oneRDD.intersection(otherRDD).collect() # removes duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1), (1, 4), (1, 4), (1, 7), (1, 1)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oneRDD.cartesian(otherRDD).collect()[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "181"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum([1,43,62,23,52])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "181"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = sc.parallelize([1,43,62,23,52])\n",
    "data.reduce(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3188536"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.reduce(lambda x, y: x * y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3188536"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 * 43 * 62 * 23 * 52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "137823683725010149883130929"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.reduce(lambda x, y: x**2 + y**2) # this does NOT compute the sum of squares of RDD elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "137823683725010149883130929"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((((1 ** 2 + 43 ** 2) ** 2 + 62 ** 2) **2 + 23 ** 2) **2 + 52 **2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8927.0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.reduce(lambda x, y: np.sqrt(x**2 + y**2)) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8927"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.array([1,43,62,23,52]) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method aggregate in module pyspark.rdd:\n",
      "\n",
      "aggregate(zeroValue, seqOp, combOp) method of pyspark.rdd.RDD instance\n",
      "    Aggregate the elements of each partition, and then the results for all\n",
      "    the partitions, using a given combine functions and a neutral \"zero\n",
      "    value.\"\n",
      "    \n",
      "    The functions C{op(t1, t2)} is allowed to modify C{t1} and return it\n",
      "    as its result value to avoid object allocation; however, it should not\n",
      "    modify C{t2}.\n",
      "    \n",
      "    The first function (seqOp) can return a different result type, U, than\n",
      "    the type of this RDD. Thus, we need one operation for merging a T into\n",
      "    an U and one operation for merging two U\n",
      "    \n",
      "    >>> seqOp = (lambda x, y: (x[0] + y, x[1] + 1))\n",
      "    >>> combOp = (lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
      "    >>> sc.parallelize([1, 2, 3, 4]).aggregate((0, 0), seqOp, combOp)\n",
      "    (10, 4)\n",
      "    >>> sc.parallelize([]).aggregate((0, 0), seqOp, combOp)\n",
      "    (0, 0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(data.aggregate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq(x,y):\n",
    "    print(x,y,\"seq\")\n",
    "    return x[0] + y, x[1] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comb(x,y):\n",
    "    print(x,y,\"comb\")\n",
    "    return x[0] + y[0], x[1] + y[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "181"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum([1,43,62,23,52])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0) (181, 5) comb\n"
     ]
    }
   ],
   "source": [
    "data = sc.parallelize([1,43,62,23,52], 1) # Try different levels of paralellism. Where are the functions printing?\n",
    "aggr = data.aggregate(zeroValue = (0,0),\n",
    "                      seqOp = seq, #\n",
    "                      combOp = comb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(181, 5)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36.2"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggr[0] / aggr[1] # average value of RDD elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## reduceByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('$APPL', 201.16), ('$AMZN', 1104.64), ('$GOOG', 706.2)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairRDD = sc.parallelize([('$APPL', 100.64), \n",
    "                          ('$APPL', 100.52), \n",
    "                          ('$GOOG', 706.2), \n",
    "                          ('$AMZN', 552.32), \n",
    "                          ('$AMZN', 552.32) ])\n",
    "\n",
    "pairRDD.reduceByKey(lambda x,y: x + y).collect() # sum of values per key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method reduceByKey in module pyspark.rdd:\n",
      "\n",
      "reduceByKey(func, numPartitions=None, partitionFunc=<function portable_hash at 0x7ff794463f28>) method of pyspark.rdd.RDD instance\n",
      "    Merge the values for each key using an associative and commutative reduce function.\n",
      "    \n",
      "    This will also perform the merging locally on each mapper before\n",
      "    sending results to a reducer, similarly to a \"combiner\" in MapReduce.\n",
      "    \n",
      "    Output will be partitioned with C{numPartitions} partitions, or\n",
      "    the default parallelism level if C{numPartitions} is not specified.\n",
      "    Default partitioner is hash-partition.\n",
      "    \n",
      "    >>> from operator import add\n",
      "    >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
      "    >>> sorted(rdd.reduceByKey(add).collect())\n",
      "    [('a', 2), ('b', 1)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(pairRDD.reduceByKey)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From https://github.com/vaquarkhan/vk-wiki-notes/wiki/reduceByKey--vs-groupBykey-vs-aggregateByKey-vs-combineByKey\n",
    "\n",
    "ReduceByKey will aggregate y key before shuffling: \n",
    "![alt text](https://camo.githubusercontent.com/516114b94193cddf7e59bdd5368d6756d30dc8b4/687474703a2f2f7777772e727578697a68616e672e636f6d2f75706c6f6164732f342f342f302f322f34343032333436352f313836363838325f6f7269672e706e67)\n",
    "\n",
    "GroupByKey will shuffle all the value key pairs as the diagrams show: \n",
    "![alt text](https://camo.githubusercontent.com/ed75baabdaee2198d3fc1390e04a5d20bcd2e484/687474703a2f2f7777772e727578697a68616e672e636f6d2f75706c6f6164732f342f342f302f322f34343032333436352f333030393135315f6f7269672e706e67)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (inner) join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = sc.textFile(os.path.join(data_folder, \"movies.csv\")).filter(lambda x: \"movie_id\" not in x).map(lambda x: x.split(\",\"))\n",
    "ratings = sc.textFile(os.path.join(data_folder, \"ratings.csv\")).filter(lambda x: \"movie_id\" not in x).map(lambda x: x.split(\",\")[1:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1193', '5'],\n",
       " ['661', '3'],\n",
       " ['914', '3'],\n",
       " ['3408', '4'],\n",
       " ['2355', '5'],\n",
       " ['1197', '3'],\n",
       " ['1287', '5'],\n",
       " ['2804', '5'],\n",
       " ['594', '4'],\n",
       " ['919', '4']]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('4', ('Waiting to Exhale (1995)', '3')),\n",
       " ('4', ('Waiting to Exhale (1995)', '3')),\n",
       " ('4', ('Waiting to Exhale (1995)', '1')),\n",
       " ('4', ('Waiting to Exhale (1995)', '4')),\n",
       " ('4', ('Waiting to Exhale (1995)', '1'))]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.join(ratings).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Accumulators\n",
    "\n",
    "This example demonstrates how to use accumulators.\n",
    "The map operations creates an RDD that contains the length of lines in the text file - and while the RDD is materialized, an accumulator keeps track of how many lines are long (longer than $30$ characters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3267"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = sc.textFile(os.path.join(data_folder, \"dbpedia.csv\"))\n",
    "long_lines = sc.accumulator(0) # create accumulator\n",
    "\n",
    "def line_data(line):\n",
    "    global long_lines # to reference an accumulator, declare it as global variable\n",
    "    length = len(line)\n",
    "    if \"abstract\" in line:\n",
    "        long_lines += 1 # update the accumulator\n",
    "    return length\n",
    "\n",
    "llengthRDD = text.map(line_data)\n",
    "llengthRDD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3266"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_lines.value # this is how we obtain the value of the accumulator in the driver program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Accumulator in module pyspark.accumulators object:\n",
      "\n",
      "class Accumulator(builtins.object)\n",
      " |  A shared variable that can be accumulated, i.e., has a commutative and associative \"add\"\n",
      " |  operation. Worker tasks on a Spark cluster can add values to an Accumulator with the C{+=}\n",
      " |  operator, but only the driver program is allowed to access its value, using C{value}.\n",
      " |  Updates from the workers get propagated automatically to the driver program.\n",
      " |  \n",
      " |  While C{SparkContext} supports accumulators for primitive data types like C{int} and\n",
      " |  C{float}, users can also define accumulators for custom types by providing a custom\n",
      " |  L{AccumulatorParam} object. Refer to the doctest of this module for an example.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __iadd__(self, term)\n",
      " |      The += operator; adds a term to this accumulator's value\n",
      " |  \n",
      " |  __init__(self, aid, value, accum_param)\n",
      " |      Create a new Accumulator with a given initial value and AccumulatorParam object\n",
      " |  \n",
      " |  __reduce__(self)\n",
      " |      Custom serialization; saves the zero value from our AccumulatorParam\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  add(self, term)\n",
      " |      Adds a term to this accumulator's value\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  value\n",
      " |      Get the accumulator's value; only usable in driver program\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(long_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warning\n",
    "\n",
    "In the example above, we update the value of an accumulator within a transformation (map). This is **not recommended**, unless for debugging purposes! The reason is that, if there are failures during the materialization of `llengthRDD`, some of its partitions will be re-computed, possibly causing the accumulator to double-count some the the long lines.\n",
    "\n",
    "It is advisable to use accumulators within actions - and particularly with the `foreach` action, as demonstrated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3267"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = sc.textFile(os.path.join(data_folder, \"dbpedia.csv\"))\n",
    "long_lines_2 = sc.accumulator(0)\n",
    "\n",
    "def line_len(line):\n",
    "    global long_lines_2\n",
    "    length = len(line)\n",
    "    if length > 30:\n",
    "        long_lines_2 += 1\n",
    "\n",
    "text.foreach(line_len)\n",
    "\n",
    "long_lines_2.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcast variable\n",
    "\n",
    "We use *broadcast variables* when many operations depend on the same large static object - e.g., a large lookup table that does not change but provides information for other operations. In such cases, we can make a broadcast variable out of the object and thus make sure that the object will be shipped to the cluster only once - and not for each of the operations we'll be using it for.\n",
    "\n",
    "The example below demonstrates the usage of broadcast variables. In this case, we make a broadcast variable out of a dictionary that represents an address table. The tablke is shipped to cluster nodes only once across multiple operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ages_catalog():\n",
    "    return {1: \"Under 18\", 18: \"18-24\", 25: \"25-34\", 35: \"35-44\", 45: \"45-49\", 50: \"50-55\", 56: \"56+\"}\n",
    "ages_catalog = sc.broadcast(load_ages_catalog())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 'Under 18'), (18, '18-24'), (50, '50-55')]\n",
      "[(35, '35-44'), (50, '50-55'), (1, 'Under 18')]\n"
     ]
    }
   ],
   "source": [
    "def find_age(age_id):\n",
    "    res = None\n",
    "    if age_id in ages_catalog.value:\n",
    "        res = ages_catalog.value[age_id]\n",
    "    return res\n",
    "\n",
    "ages = sc.parallelize([1,18,50])\n",
    "pairRDD = ages.map(lambda age_id: (age_id, find_age(age_id)))\n",
    "print(pairRDD.collect())\n",
    "\n",
    "other_ages = sc.parallelize([35, 50, 1])\n",
    "pairRDD = other_ages.map(lambda age_id: (age_id, find_age(age_id)))\n",
    "print(pairRDD.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High-level structured"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There are many ways to solve an information need. For example, let's try two different queries for finding the number of ratings female users gave to each movie:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_fr = sc.textFile(os.path.join(data_folder, \"ratings.csv\")).map(lambda x: (x.split(\",\")[0], x.split(\",\")[1],1))\n",
    "f_users = sc.textFile(os.path.join(data_folder, \"users.csv\")).map(lambda x: (x.split(\",\")[0],x.split(\",\")[1])).filter(lambda x: x[1] == \"F\")\n",
    "ratings_fr_res = ratings_fr.join(f_users).map(lambda x: (x[1][0], 1)).reduceByKey(lambda x,y: x + y).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_rf = sc.textFile(os.path.join(data_folder, \"ratings.csv\")).map(lambda x: (x.split(\",\")[0], x.split(\",\")[1],1))\n",
    "users = sc.textFile(os.path.join(data_folder, \"users.csv\")).map(lambda x: (x.split(\",\")[0],x.split(\",\")[1]))\n",
    "ratings_rf_res = ratings_rf.join(users).filter(lambda x: x[1][1] == \"F\").map(lambda x: (x[1][0], 1)).reduceByKey(lambda x,y: x + y).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What happened? Let's find out in Spark UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Working with RDD allows developers to have more freedom. \n",
    "- However, this is not recommended. There are new high-lever structured API that optimize many steps of the data transformations. \n",
    "- In general, it is pointless trying to beat a query optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_sc = sql.SparkSession(sparkContext=sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_file = os.path.join(data_folder, \"ratings.csv\")\n",
    "ratings = sql_sc.read.option(\"inferSchema\", \"true\").option(\"header\", \"true\").csv(ratings_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.createOrReplaceTempView(\"ratings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_file = os.path.join(data_folder, \"users.csv\")\n",
    "users = sql_sc.read.option(\"inferSchema\", \"true\").option(\"header\", \"true\").csv(users_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "users.createOrReplaceTempView(\"users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlWay = sql_sc.sql(\"\"\"\n",
    "SELECT r.movie_id, count(r.rating)\n",
    "FROM ratings r\n",
    "INNER JOIN users u on r.user_id = u.user_id\n",
    "WHERE u.gender = 'F'\n",
    "GROUP BY r.movie_id\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrameWay = ratings.join(users, ratings.user_id == users.user_id).filter(users.gender == 'F') \\\n",
    "  .groupBy(ratings.movie_id).agg({\"rating\": \"count\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "519 ms ± 131 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit sqlWay.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "422 ms ± 44.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit dataFrameWay.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*HashAggregate(keys=[movie_id#13], functions=[count(rating#14)])\n",
      "+- Exchange hashpartitioning(movie_id#13, 200)\n",
      "   +- *HashAggregate(keys=[movie_id#13], functions=[partial_count(rating#14)])\n",
      "      +- *Project [movie_id#13, rating#14]\n",
      "         +- *BroadcastHashJoin [user_id#12], [user_id#34], Inner, BuildRight\n",
      "            :- *Project [user_id#12, movie_id#13, rating#14]\n",
      "            :  +- *Filter isnotnull(user_id#12)\n",
      "            :     +- *FileScan csv [user_id#12,movie_id#13,rating#14] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/home/ubuntu/movielens_v2/movielens/ratings.csv], PartitionFilters: [], PushedFilters: [IsNotNull(user_id)], ReadSchema: struct<user_id:int,movie_id:int,rating:int>\n",
      "            +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n",
      "               +- *Project [user_id#34]\n",
      "                  +- *Filter ((isnotnull(gender#35) && (gender#35 = F)) && isnotnull(user_id#34))\n",
      "                     +- *FileScan csv [user_id#34,gender#35] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/home/ubuntu/movielens_v2/movielens/users.csv], PartitionFilters: [], PushedFilters: [IsNotNull(gender), EqualTo(gender,F), IsNotNull(user_id)], ReadSchema: struct<user_id:int,gender:string>\n"
     ]
    }
   ],
   "source": [
    "sqlWay.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*HashAggregate(keys=[movie_id#13], functions=[count(rating#14)])\n",
      "+- Exchange hashpartitioning(movie_id#13, 200)\n",
      "   +- *HashAggregate(keys=[movie_id#13], functions=[partial_count(rating#14)])\n",
      "      +- *Project [movie_id#13, rating#14]\n",
      "         +- *BroadcastHashJoin [user_id#12], [user_id#34], Inner, BuildRight\n",
      "            :- *Project [user_id#12, movie_id#13, rating#14]\n",
      "            :  +- *Filter isnotnull(user_id#12)\n",
      "            :     +- *FileScan csv [user_id#12,movie_id#13,rating#14] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/home/ubuntu/movielens_v2/movielens/ratings.csv], PartitionFilters: [], PushedFilters: [IsNotNull(user_id)], ReadSchema: struct<user_id:int,movie_id:int,rating:int>\n",
      "            +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n",
      "               +- *Project [user_id#34]\n",
      "                  +- *Filter ((isnotnull(gender#35) && (gender#35 = F)) && isnotnull(user_id#34))\n",
      "                     +- *FileScan csv [user_id#34,gender#35] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/home/ubuntu/movielens_v2/movielens/users.csv], PartitionFilters: [], PushedFilters: [IsNotNull(gender), EqualTo(gender,F), IsNotNull(user_id)], ReadSchema: struct<user_id:int,gender:string>\n"
     ]
    }
   ],
   "source": [
    "dataFrameWay.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming and Structured Streaming\n",
    "#### Check the examples in:\n",
    "- /home/ubuntu/software/spark/examples/src/main/python/streaming/network_wordcount.py\n",
    "- /home/ubuntu/software/spark/examples/src/main/python/streaming/sql_network_wordcount.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### That's all "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
